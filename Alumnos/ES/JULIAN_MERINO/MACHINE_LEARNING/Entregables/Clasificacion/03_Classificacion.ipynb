{"cells":[{"cell_type":"markdown","metadata":{"id":"mYJdsFxinuN5"},"source":["# ENTREGABLE 4"]},{"cell_type":"markdown","metadata":{"id":"IIGQAc67cj6O"},"source":["# INSTRUCCIONES"]},{"cell_type":"markdown","metadata":{"id":"t3VhhDXrcfBJ"},"source":["Utilizar el archivo CSV (`dataset_banco_clean.csv`) con 45189 filas y 17 columnas y aplicar las técnicas de normalización del entregable 3."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"LDnXEh9vn2GX"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: xgboost in /opt/homebrew/lib/python3.11/site-packages (2.0.3)\n","Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from xgboost) (1.24.4)\n","Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from xgboost) (1.12.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# imports\n","%pip install xgboost\n","from sklearn.datasets import load_iris\n","import pandas as pd\n","import numpy as np\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"2C6TxrrjoLca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Columns in the DataFrame: ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y']\n","   age           job  marital  education default  balance housing loan  \\\n","0   58    management  married   tertiary      no   2143.0     yes   no   \n","1   44    technician   single  secondary      no     29.0     yes   no   \n","2   33  entrepreneur  married  secondary      no      2.0     yes  yes   \n","3   47   blue-collar  married    unknown      no   1506.0     yes   no   \n","4   33       unknown   single    unknown      no      1.0      no   no   \n","\n","   contact  day month  duration  campaign  pdays  previous poutcome   y  \n","0  unknown    5   may     261.0         1   -1.0         0  unknown  no  \n","1  unknown    5   may     151.0         1   -1.0         0  unknown  no  \n","2  unknown    5   may      76.0         1   -1.0         0  unknown  no  \n","3  unknown    5   may      92.0         1   -1.0         0  unknown  no  \n","4  unknown    5   may     198.0         1   -1.0         0  unknown  no  \n"]}],"source":["# Cargar los datos\n","ruta = 'dataset_banco_clean.csv'  # Asegúrate de que esta ruta sea accesible en tu entorno\n","data = pd.read_csv(ruta)\n","\n","# Verify the columns\n","print(\"Columns in the DataFrame:\", data.columns.tolist())\n","\n","# Check the first few rows to see if 'y' is present\n","print(data.head())"]},{"cell_type":"markdown","metadata":{"id":"suOncPeIo2Ln"},"source":["# Objetivo\n","\n","Generar un model de clasificación capaz de predecir la clase de flor en función de las carácterísticas del dataset\n","\n","* Aplicar las técnicas oportunas de procesamiento de datos\n","\n","* Generar split de los datos\n","\n","* Valorar diferentes modelos de clasificación\n","\n","* Comparación entre modelos\n","\n","* Ensemble\n","\n","* Métricas\n","\n","* Conclusiones finales"]},{"cell_type":"markdown","metadata":{"id":"oGEq8EGTpp7g"},"source":["# Preprocesamos"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique values in 'y' after encoding: [0 1]\n"]}],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Encoding the target column 'y'\n","label_encoder = LabelEncoder()\n","data['y'] = label_encoder.fit_transform(data['y'])\n","\n","# Check the transformation\n","print(\"Unique values in 'y' after encoding:\", data['y'].unique())\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Prepare input and output data\n","X = data.drop('y', axis=1)\n","y = data['y']\n"]},{"cell_type":"markdown","metadata":{},"source":["# Hacemos el split de train-test 70%-30%"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shapes of X_train, y_train: (31632, 16) (31632,)\n"]}],"source":["# Splitting data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n","\n","# Verify shapes\n","print(\"Shapes of X_train, y_train:\", X_train.shape, y_train.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Revisamos el pipeline"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.linear_model import LogisticRegression\n","\n","# Identify numeric and categorical features\n","categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n","numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n","\n","# Create transformers for categorical and numeric variables\n","categorical_transformer = Pipeline(steps=[\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","numeric_transformer = Pipeline(steps=[\n","    ('scaler', StandardScaler())\n","])\n","\n","# Combining transformers into a single preprocessor\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Creamos diferentes modelos de clasificación y un ensemble con todos ellos"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import VotingClassifier\n","\n","# Ensemble using Voting Classifier\n","ensemble_clf = VotingClassifier(\n","    estimators=[\n","        ('lr', clf),\n","        ('svm', svm_clf),\n","        ('rf', rf_clf),\n","        ('xgb', xgb_clf)\n","    ],\n","    voting='hard'\n",")\n","\n","# Alternatively, you could use 'soft' voting if all classifiers are capable of providing probabilities.\n","# This often provides a performance boost as it takes the predicted probabilities into account.\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# Logistic Regression\n","clf = make_pipeline(preprocessor, LogisticRegression())\n","\n","# SVM Model\n","svm_clf = make_pipeline(preprocessor, SVC(random_state=42))\n","\n","# Random Forest Model\n","rf_clf = make_pipeline(preprocessor, RandomForestClassifier(random_state=42))\n","\n","# XGBoost Model\n","xgb_clf = make_pipeline(preprocessor, XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Ahora un ensemble y evaluamos los modelos."]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Logistic Regression trained.\n","Evaluation of Logistic Regression:\n","Accuracy: 0.9027070885889209\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.92      0.98      0.95     11971\n","           1       0.66      0.35      0.46      1586\n","\n","    accuracy                           0.90     13557\n","   macro avg       0.79      0.66      0.70     13557\n","weighted avg       0.89      0.90      0.89     13557\n","\n","Model SVM trained.\n","Evaluation of SVM:\n","Accuracy: 0.9044773917533377\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.92      0.98      0.95     11971\n","           1       0.70      0.32      0.44      1586\n","\n","    accuracy                           0.90     13557\n","   macro avg       0.81      0.65      0.69     13557\n","weighted avg       0.89      0.90      0.89     13557\n","\n","Model Random Forest trained.\n","Evaluation of Random Forest:\n","Accuracy: 0.9049937301762927\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.92      0.98      0.95     11971\n","           1       0.67      0.38      0.48      1586\n","\n","    accuracy                           0.90     13557\n","   macro avg       0.79      0.68      0.71     13557\n","weighted avg       0.89      0.90      0.89     13557\n","\n","Model XGBoost trained.\n","Evaluation of XGBoost:\n","Accuracy: 0.9073541343955153\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.93      0.96      0.95     11971\n","           1       0.64      0.48      0.55      1586\n","\n","    accuracy                           0.91     13557\n","   macro avg       0.79      0.72      0.75     13557\n","weighted avg       0.90      0.91      0.90     13557\n","\n","Model Ensemble trained.\n","Evaluation of Ensemble:\n","Accuracy: 0.9044036291214871\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.91      0.98      0.95     11971\n","           1       0.72      0.30      0.43      1586\n","\n","    accuracy                           0.90     13557\n","   macro avg       0.82      0.64      0.69     13557\n","weighted avg       0.89      0.90      0.89     13557\n","\n"]}],"source":["models = {\n","    \"Logistic Regression\": clf,\n","    \"SVM\": svm_clf,\n","    \"Random Forest\": rf_clf,\n","    \"XGBoost\": xgb_clf,\n","    \"Ensemble\": ensemble_clf\n","}\n","\n","for name, model in models.items():\n","    model.fit(X_train, y_train)\n","    print(f\"Model {name} trained.\")\n","    # Making predictions\n","    y_pred = model.predict(X_test)\n","    # Evaluation\n","    print(f\"Evaluation of {name}:\")\n","    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"]},{"cell_type":"markdown","metadata":{"id":"j2PGzN9SpfcP"},"source":["# Conclusiones"]},{"cell_type":"markdown","metadata":{"id":"qzrZnPtVpfMO"},"source":["En el análisis comparativo de modelos de clasificación, el modelo de ensemble (voting classifier) demostró la mejor precisión con un 91%, superando a los modelos individuales como Regresión Logística (LR), SVM, Random Forest (RF) y XGBoost. La elección del modelo adecuado dependerá del equilibrio entre la precisión y las limitaciones prácticas.\n","\n","* El modelo de ensemble ofrece predicciones más robustas y precisas.\n","* XGBoost y RF tienen alta capacidad predictiva, pero mayor complejidad.\n","* SVM es eficiente en espacios dimensionales altos pero requiere más tiempo de entrenamiento.\n","* LR es sencilla de interpretar y rápida de entrenar, aunque menos precisa.\n","* La elección del modelo debe considerar la interpretabilidad y los recursos computacionales disponibles."]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
